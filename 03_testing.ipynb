{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a80f27",
   "metadata": {},
   "source": [
    "# Testing the solution\n",
    "Now it's time to test our new container. We can pull the container based on the name we gave it (`algorithms_name`) and pull it from ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf365ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client=boto3.client('sts')\n",
    "account=client.get_caller_identity()['Account']\n",
    "\n",
    "my_session=boto3.session.Session()\n",
    "region=my_session.region_name\n",
    "\n",
    "algorithm_name=\"huggingface-pytorch-inference-extended\"\n",
    "tag=\"1.10.2-transformers4.24.0-gpu-py38-cu113-ubuntu20.04\"\n",
    "ecr_image='{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account, region, algorithm_name, tag)\n",
    "\n",
    "ecr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6f8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e9853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = {\n",
    "    'HF_MODEL_ID':'bigscience/bloom-560m',\n",
    "    'HF_TASK':'text-generation'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be633970",
   "metadata": {},
   "source": [
    "All we need to do when creating the `Model` object is to tell it that we want to use our own container. Note that in this case we don't need to provide the info about version numbers anymore (which is only used to identify the appropraite container) since we specify the container we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5afc72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=ecr_image,\n",
    "    env=hub,\n",
    "    role=role,\n",
    "#     transformers_version=\"4.17\",\n",
    "#     pytorch_version=\"1.10\",\n",
    "#     py_version=\"py38\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e07eda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30bdf67",
   "metadata": {},
   "source": [
    "The model deployed to an endpoint, and now we can actually send an inference request and get the expected response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b151990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'And this is is the solution to the problem.\\nI have a problem with the following code.\\n#include'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"inputs\": \"And this is is the solution\"}\n",
    "\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c088831",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
